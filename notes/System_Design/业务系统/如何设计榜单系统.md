[toc]



## 1. 需求是什么
- 需求单
- 原型图
## 2. 为什么要做这个需求
- 用于解决什么问题
- 需求是不是可以不做
- 能不能简化下需求

## 3. 需求分析
理解概念需要结合系统举例子
### 3.1. 原有流程是怎样的
找运营或者产品演示流程
读写流程、B端C端流程
### 3.2. 新流程是怎样的
1. 系统中的角色以及每个角色可以做什么
    - [用例图.md](../Software_Engineering/建模/用例图.md)
2. 每个角色做一个事情的流程是怎样的
    - [流程图.md](../Software_Engineering/建模/流程图.md)
    - [时序图.md](../Software_Engineering/建模/时序图.md)
### 3.3. QPS估算
假设主播人数为1W，头部大房间在线人数为10W
## 4. 方案设计


### 4.1. 存储设计
覆盖：`zadd 1_2_3 1 zsk`
增加或者减少：`zincrby 1_2_3 5 zsk`
按分数倒序获取：`zrevrange 1_2_3 0 -1 withscores`

### 4.2. 接口设计

```go
//CMD=1020
//通用排行榜读取 limit <500
message GetRankListReq {
    required uint32 actId = 1; //活动ID
    required RankIdAndObjId rankObj = 2; //榜单对象ID
    required uint32 start = 3; //返回信息
    required uint32 limit = 4; //返回信息
    optional uint64 t_time = 5; //指定时间位置的榜单,不填以当前系统时间
    optional string tid = 6; //需要我自已的排名,填自已的tid
    //性能优化选项,需要的数据填充读取策略过滤,不填则拉全部,填RAW,原始数据, INFO,只要资料, RANK_COMPARE,排名比较,ALL_GROUP,SPEC_GROUP (SPEC_GROUP 与 ALL_GROUP 不兼容)
    repeated string needDataFilter = 7; //返回信息
    optional bool need_total = 8; //需要榜单总成员数.
    repeated string groups = 9; //读取的指定group 不超过10个
    repeated string ext_json = 10; //额外信息
    optional uint32 order_type = 11; //榜单顺序，默认降序， 0=降序， 1=升序
    optional bool noCache = 12; // 读取无缓存数据. 需置为true
}

//通用排行榜读取返回
message GetRankListRsp {
    required uint32 actId = 1; //活动ID
    required RankIdAndObjId rankObj = 2; //榜单对象ID
    required string rank_name = 4; //榜单名称
    repeated RankMemberkInfo rank_list = 10; //返回信息
    optional RankMemberkInfo rank_my = 11; //我的排名信息
    optional uint32 rank_total = 12; //榜单总成员数
    repeated RankMemberkInfo group_rank_list = 13; //只在分组模式榜单返回,A,B,C组排行,注意是按分值排名的MemberId才是组ID
    optional string locked = 14; //榜单是否已锁定, locking 统计中, nolock ,  locked
    repeated string ext_json = 15; //扩展属性字段,如升级奖励加成倒计时
    repeated SingleGroupRank all_group_rank_list = 16; //所有组的榜单
}

//CMD=1003
//给指定业务功能榜单更新积分
message UpdateRankScoreReq {
    required uint32 appid = 1; //业务ID
    required uint32 actId = 2; //活动ID
    required string xcode = 3; //验证码
    required RankIdAndObjId rankObj = 4; //榜单对象
    repeated MemberAndScore data = 5; //成员 可以多个, <100个
    optional uint64 t_time = 6; //指定时间位置的榜单,不填以当前系统时间
    optional uint32 update_type = 7; //榜单更新类型 默认覆盖 0=覆盖  1=incrby
    optional string msgId = 8; //唯一流水ID,操作流水订单号,用于可幂等重试
    optional uint64 plusOrminus = 9; // 加分=0; 减分=1;

}

message UpdateRankScoreRsp {
    repeated MemberAndScore data = 3; //成员
}

//榜单类型
message RankIdAndObjId {
    required uint32 rankId = 1; //榜单ID
    optional string objId = 2; //二维榜单对象ID
}

message MemberAndScore {
    required string member_id = 1; //成员ID
    required uint64 score = 2; //分值
    optional uint32 errCode = 3; // 返回时 错误码
    optional string data = 4; //  资料
	optional uint64 result_score = 5; //加分后的总值
}

//CMD=1005
//给指定业务功能榜单删除成员
message DeleteRankMemberReq {
    required uint32 appid = 1; //业务ID
    required uint32 actId = 2; //活动ID
    required string xcode = 3; //验证码
    required RankIdAndObjId rankObj = 4; //榜单对象
    repeated MemberAndScore data = 5; //成员 可以多个, <100个
    optional uint64 t_time = 6; //指定时间位置的榜单,不填以当前系统时间
}

message DeleteRankMemberRsp {

}
```

#### 4.2.1. 根据排名拉取榜单详情
- req
    - appid
    - rankid
    - subrankid
    - start
    - limit
    - me
- rsp
    - list
        - member
        - score
    - me
        - member
        - score
#### 4.2.2. 更新榜单
- req
    - appid
    - rankid
    - subrankid
    - list
        - member
        - score
        - orderNo（幂等）
        - opType(覆盖 加)
- rsp
    - list
        - member
        - score

### 4.3. 架构设计
综合考虑[安全性](../Safe/安全.md)、[高并发](如何设计高并发系统.md)、[高可用](如何设计高可用系统.md)、[可维护](如何设计可维护系统.md)

1. 拆分[微服务.md](../Software_Engineering/Architecture/微服务/微服务.md)
2. 画出[应用架构图](../Software_Engineering/建模/架构图.md)
3. 画出[时序图](../Software_Engineering/建模/时序图.md)

### 4.4. 代码设计
#### 4.4.1. update
pipeline+evalsha lua+兜底eval lua
score是`分数+'.'+9 9999 9999 9999-当前时间戳（1 6523 5401 7172）`保证相同分数的情况下；先达到的排到前面

```go
-- appid_rankid_subrankid , zsk 3.5 uuid 1 1800
local rankListKey = KEYS[1]
local member = ARGV[1]
local score = ARGV[2]
local orderNo = ARGV[3]
local opType = tonumber(ARGV[4])
local expireTs = tonumber(ARGV[5])

local function markOrderProcessed(orderNo, expireTs)
    return redis.call("SET", orderNo, 1, "EX", expireTs, "NX")
end

local function isRankListKeyExists(rankListKey)
    return redis.call("EXISTS", rankListKey) == 1
end

local function setMemberScore(rankListKey, member, score)
    return redis.call("ZADD", rankListKey, score, member)
end

local function incrOrDecrMemberScore(rankListKey, member, score)
    return redis.call("ZINCRBY", rankListKey, score, member)
end

local function setRankListKeyExpireTs(rankListKey, expireTs)
    return redis.call("EXPIRE", rankListKey, expireTs)
end

local function getMemberScore(rankListKey, member)
    return redis.call("ZSCORE", rankListKey, member)
end

local function main()
    if opType ~= 1 and opType ~= 2 then
        return {10002, 0}
    end

    if not markOrderProcessed(orderNo, expireTs) then
        return {10001, 0}
    end

    local isRankListKeyExists = isRankListKeyExists(rankListKey)
    if opType == 1 then
        setMemberScore(rankListKey, member, score)
    elseif opType == 2 then
        incrOrDecrMemberScore(rankListKey, member, score)
    end
    if not isRankListKeyExists then
       setRankListKeyExpireTs(rankListKey, expireTs)
    end
    local memberScore = getMemberScore(rankListKey, member)
    return {0, memberScore}
end

return main()
```
#### 4.4.2. get


- getByRank
```
// 按分数逆序排序
zrevrange 1_2_3 start start+limit-1 withscores
// 按分数正序排序
zrange 1_2_3 start start+limit-1 withscores
```
删除小数
`strings.Split("", ".")[0]`


- getByMember

```lua
-- appid_rankid_subrankid , zsk 2
local rankListKey = KEYS[1]
local member = ARGV[1]
local order = ARGV[2]

local function getMemberRank(rankListKey, member, order)
    if order == 1 then
        return redis.call("ZRANK", rankListKey, member)
    else
        return redis.call("ZREVRANK", rankListKey, member)    
    end
end

local function getMemberScore(rankListKey, member)
    return redis.call("ZSCORE", rankListKey, member)
end

local function main()
    local score = getMemberScore(rankListKey, member)
    if not score then
        return {-1, 0, 0}
    end

    local rank = getMemberRank(rankListKey, member, order)
    return {0, rank, score}
end

return main()
```

getMember和getRank也可以封装到Lua中
### 4.5. 幂等性设计
写操作加个orderNo, 由业务方生成保证写操作重试时幂等
和上榜操作一起封装到Lua脚本中
### 4.6. 如何应对高并发
首先说下排行榜的业务特点，
1. 作用是一是活跃气氛，二是给主播发红包，那么不需要展示整个榜单，跟产品商量了下只展示前50名；
2. 属于读多写少的业务

高并发可以分成两块，高并发写和高并发读
#### 4.6.1. 如何应对高并发写
一方面用了Kafka，另一方面榜单是个读多写少的业务，所以写不是优化的重点
#### 4.6.2. 如何应对高并发读
高并发读无非就是用缓存，问题是怎么用。

首先看下榜单展示的逻辑，从Redis中拉取榜单前10名、个人分数以及排名，RPC调用聚合用户头像和昵称
假设并发不高，那么肯定是从Redis读取榜单排名，以及个人分数，然后RPC调用聚合用户头像昵称
但是并发高的情况下，必须考虑用本地缓存了



首先是用户资料，缓存方案选型如下：

1. 不回源。就是把用户资料服务的所有用户都拉取过来，这个数据量太大不现实；
2. 回源
    - 读：read-through：聚合用户资料时先从本地内存中取，没有的则从用户资料服务拉取缓存到本地。
        - 缓存命中率低：可以使用proxy+一致性Hash。但是一致性Hash会有单点过热问题，扩容无法解决
    - 写：用户资料更新的时候我们这边没有事件，所以这个方案用不了，但是考虑到用户资料基本不会变化，所以靠超时来更新缓存问题不大

最终考虑到榜单只展示前50名，并且头像和昵称变化不大，决定使用read-through，同时为了提高缓存命中率，一方面启动时对Top的热榜单缓存到本地预热，~~另一方面分页请求构建缓存太慢，而且为了保证热点数据的缓存命中率，最后决定改造成定时任务拉取Top榜单更新缓存~~
~~为什么需要定时加载热数据？LRU可能受冷数据的影响导致热数据被淘汰出缓存，LFU可能受上次热数据的影响导致现在的热数据被淘汰出缓存，综合考虑定时加载~~
缓存大小：5000场直播，每个100人，每人150字节，那么缓存大小确定为5000*100，占用75m。缓存时间用户平均活跃时间，但是没有统计数据那么就用直播时长好了

其次是榜单和个人分数，缓存方案选型如下：

1. 不回源：
    - 全量缓存：数据量不大的情况下定时全量加载到缓存，如果机器数为10，主播数为1W，那么对Redis的QPS就是1W*10=10W
2. 回源：
    - 读：read-through：读的时候缓存中没有那么从Redis中读，然后放入本地LRU缓存；有的话直接从本地LRU缓存中读取返回。但有几个问题：
        - 分页。A用户0-10打到A节点缓存，10-20打到B节点缓存，20-30打到C节点缓存；B用户0-10打到B节点缓存，10-20打到C节点缓存，20-30打到A节点缓存，虽然对于热榜单利用LRU-cache命中率迟早上来，可是命中率还是太低了；并且前10的分数可能变动很频繁，不适合缓存


最终考虑到缓存命中率和变动频繁，不进行缓存

首先缓存不合适，那么可以先对单key压测看是否满足需求，满足的话就不需要过度优化了，所以单独针对Lua进行压测。过程参考[频控.md](../../公司/广告/频控.md)
其次代码层面不缓存，不代表基础组件没有缓存，于是查了下腾讯云文档是否有相应的功能，Redis全球复制出来了，过程参考[频控.md](../../公司/广告/频控.md)
最后有些大房间PCU很高，可能导致榜单访问也很高，会导致Redis热key问题，这种得考虑缓存
### 4.7. 热榜单问题
- 如何发现hot key
    1. 手动配置。直播场景下是可以预知的
    2. 腾讯云Redis监控可以每5s统计一次访问TopN的key。我们这边分布式定时任务每1s轮询一次他的API查询热key
    - [云数据库 Redis 5秒监控更新说明\-操作指南\-文档中心\-腾讯云\-腾讯云](https://cloud.tencent.com/document/product/239/48573)
    - [云数据库 Redis 查询实例热Key\-API 文档\-文档中心\-腾讯云\-腾讯云](https://cloud.tencent.com/document/product/239/38920)
- 如何处理hot key
    - 1W QPS以下的Redis抗
    - 1W QPS以上的双写Redis和内存，用内存抗。预配置启动时拉Redis总邀请数，通过邀请数对比是否重放完然后用channel通知server对外服务，但是定时监控热key加载到缓存需要加个字段是否准备完成




综合考虑：

1. 超大房间不需要榜单的预配置屏蔽，
2. 超大房间需要榜单的预配置双写Redis和内存
    1. 进房消息转榜单消息，topic为`update_rank`
    2. 节点使用监听`update_rank`topic，收到消息后更新Redis
    3. 更新Redis成功后，如果榜单对应的主播ID为热主播，那么发送更新Kafka本地缓存消息，topic为`update_cache_榜单ID`
    4. 节点启动的时候从配置拉取热主播UID，再去Redis拉取对应的正在进行的榜单ID以及当前总邀请数，然后使用随机的group监听`update_cache_榜单ID`，批量消费消息重放至内存，直到重放的邀请数>=从Redis拉取的总邀请数，才用channel告知server启动服务。最后如果有消息，更新本地缓存
        - Kafka批量消费数量问题：[java \- How to increase the number of messages consumed by Spring Kafka Consumer in each batch? \- Stack Overflow](https://stackoverflow.com/questions/50283011/how-to-increase-the-number-of-messages-consumed-by-spring-kafka-consumer-in-each)
        - 为什么不直接用Redis读写分离抗高并发，成本问题，Redis扩容成本远大于业务服务器。比如1核心1G的情况下对比，Redis比机器贵5倍
        - 节点重启拉Redis之后到对外服务消费Kafka时有段延迟，消息可能丢失，所以改用消费Kafka消息事件驱动
        - sarama不支持正则正则匹配Topic消费，不过confluent-kafka-go支持正则匹配Topic消费`^topic.*`。
        - 为什么不对所有或者Top榜单双写？所有榜单数据量大内存存不下，假设同时在线5000场，每个都开启了榜单，每个榜单10W个人（占用20m内存），那么总共占用5000*20m=100g；Top榜单设计复杂
3. 大房间依靠监控热key缓存榜单前50，查个人排名穿透到Redis
4. 小房间查榜单和个人都穿透Redis



```go

type RankList struct {
	*zset.SortedSet
	orderNos     map[string]bool
	totalInvites int64
	sync.RWMutex
}

func (r *RankList) String() string {
	marshal, _ := json.Marshal(r)
	return string(marshal)
}

func NewRankList() *RankList {
	return &RankList{
		SortedSet: zset.New(),
		orderNos:  make(map[string]bool, 0),
	}
}

func (r *RankList) GetMember(uid string) *Member {
	uidInt, _ := strconv.Atoi(uid)
	r.RLock()
	defer r.RUnlock()
	data, ok := r.GetData(int64(uidInt))
	if ok {
		return data.(*Member)
	}
	return nil
}

func (r *RankList) GetMembers(start, num int64) []*Member {
	members := make([]*Member, 0, num)
	r.RLock()
	defer r.RUnlock()
	r.RevRange(start, start+num, func(score float64, key int64, data interface{}) {
		member, ok := data.(*Member)
		if ok {
			members = append(members, member)
		}
	})
	return members
}
func (r *RankList) GetTotalInvites() int64 {
	r.RLock()
	defer r.RUnlock()
	return r.totalInvites
}

func (r *RankList) Update(msgs ...*UpdateCacheMsg) {
	r.Lock()
	defer r.Unlock()
	for _, msg := range msgs {
		orderNo := buildOrderNo(msg)
		isExists := r.orderNos[orderNo]
		if isExists {
			continue
		}
		uid, _ := strconv.Atoi(msg.InviterUID)
		data, ok := r.GetData(int64(uid))
		if ok {
			m := data.(*Member)
			m.Score += 1
			r.IncrBy(1, int64(uid))
		} else {
			m := &Member{
				UID:   msg.InviterUID,
				Score: 1,
			}
			r.Set(1, int64(uid), m)
		}
		r.totalInvites++
		r.orderNos[orderNo] = true
	}
}

func buildOrderNo(msg *UpdateCacheMsg) string {
	return fmt.Sprintf("%v_%v_%v", msg.RankID, msg.InviterUID, msg.InviteeUID)
}
```
### 4.8. 大榜单问题
- 如何发现大key
    1. 手动配置。直播场景下是可以预知的
    2. 监控。腾讯云Redis监控可以每5s统计一次member数目TopN的key。我们这边分布式定时任务每1s轮询一次他的API查询大key
- 如何处理大key
    - 拆分。
        - 写：接收Kafka写榜单的时候，查询下是否大key，是的话那么首先计算hash(member)得出suffix，拼接到榜单key后面，即key_suffix，然后就是zset key_suffix member score，最后记录下key_suffix到set中
        - 读：
            - 方案一：读取榜单的时候，查询是否大key，是的话查询set中的所有key_suffix，并行查询前10，在内存聚合排序出前10返回。实时性高，但是效率低
            - 方案二：定时任务，查询set中的所有key_suffix，在内存聚合排序，缓存到本地。读取榜单的时候直接读取本地内存。实时性低，效率高
    - 个人的分数和排名很可能不在榜单前几中，那么还是得穿透Redis的榜单key查询，怎么办呢？写榜单的时候同时查询个人的分数和排名，单独写入到Hash结构中，这样子读个人排名依然会穿透到Redis，但是不会对榜单的key造成压力

### 4.9. 对账模块
业务方落DB，通用榜单落DB
对账模块拉取最近5秒业务方的cdb流水 与我们的cdb流水做差异性对账
一旦返现差异消息，就做重发消息，去做幂等性重复执行修复数据。
## 5. 工作量评估
- 一个接口评估0.5-2天
## 6. 开发


## 7. 测试
- Redis压测：
    - udpate QPS大概5.5W
    - get 榜单10名 QPS大概7.2W
- Kafka压测，一个partitionQPS大概10W
```go
bin/kafka-consumer-perf-test.sh --broker-list 11.151.219.30:19092 --topic update_cache --fetch-size 120000 --messages 1000000
start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec
2022-05-31 19:43:34:040, 2022-05-31 19:43:39:361, 92.4757, 17.3794, 538717, 101243.5632, 10, 5311, 17.4121, 101434.1932
```
- 服务压测：
    - 本地内存
        - 1核2G的机器，50ms内返回：
            - zset拉取100名， QPS大概1.1W，改用GC插件1.4W
            - zset拉取10名， QPS大概3.4W，改用GC插件3.7W
        - 5.7W zset 占用内存9m，启动重放Kafka事件耗时大概3s不到
        - 21W zset 占用内存，启动重放Kafka事件耗时大概6s，每秒大概3.5W，那么启动时计算一下启动时间作为超时即可
    - Redis
        - 1核2G的机器，50ms内返回：大概仅能用到Redis实际QPS的1/4，即1.3W

## 8. 发布
1. 服务发布Checklist
2. 上线部署
    - [部署图.md](建模/部署图.md)
    - `机器数目=系统预计QPS/每台机器QPS`。每台机器的QPS可参考[压力测试.md](../Test/压力测试.md)

## 9. 运维
### 9.1. Redis全球复制
[频控.md](../../公司/广告/频控.md)
## 10. 优化

## 11. 总结

1. 方案对比
2. 遇到的问题以及怎么解决的
3. 设计中的亮点
    - 为什么引入XXX组件
3. 痛点梳理与改进措施
    - 请求量、数据量扩大N倍怎么处理
    - [重构.md](重构.md)
## 12. 参考
- [使用redis进行排行榜的小秘诀 \- SegmentFault 思否](https://segmentfault.com/a/1190000018636887)
